{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.744093Z",
     "start_time": "2025-04-20T13:02:58.632642Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "\"\"\" Imports \"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Загрузка необходимых ресурсов\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('book')\n",
    "nltk.download('punkt') \n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('reuters')\n",
    "\n",
    "from nltk import NaiveBayesClassifier, classify, DecisionTreeClassifier\n",
    "from nltk.book import *\n",
    "from nltk.text import Text as nltk_text_type\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n",
      "[nltk_data] Downloading package punkt to /home/kapuchinka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /home/kapuchinka/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Модуль Токенизация Слов",
   "id": "3b27c0cb9debd297"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.748368Z",
     "start_time": "2025-04-20T13:03:02.745350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenization_words(text: nltk_text_type, language: str = 'english'):\n",
    "    word_list = text.tokens\n",
    "    text_stroke = \" \".join(word_list)\n",
    "    words = word_tokenize(text_stroke, language)\n",
    "    words_tokenize = [word for word in words if re.match(r'\\w+', word)]   \n",
    "    \n",
    "    return words_tokenize"
   ],
   "id": "5b85a151fe34a816",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Модуль удаления Стоп-Слов",
   "id": "5db0370f402609aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.758883Z",
     "start_time": "2025-04-20T13:03:02.749300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_stopwords(words_tokenize, language: str = 'english'):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    words_without_stopwords = [word for word in words_tokenize if word.lower() not in stop_words]     \n",
    "    \n",
    "    return words_without_stopwords"
   ],
   "id": "a49ee15171a0151c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Лемматизация",
   "id": "620f824a0d241576"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.763124Z",
     "start_time": "2025-04-20T13:03:02.759797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Преобразует POS-тег NLTK в формат WordNet.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatizate_text(filtered_words):\n",
    "    pos_tags = pos_tag(filtered_words)\n",
    "    \n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(word.lower(), pos=wordnet_pos)\n",
    "        lemmatized_tokens.append(lemma)\n",
    "\n",
    "    return lemmatized_tokens"
   ],
   "id": "d2257243597ecee0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.767529Z",
     "start_time": "2025-04-20T13:03:02.764456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filtration_documents(corpus):\n",
    "    local_documents = []\n",
    "    for category in corpus.categories():\n",
    "        for fileid in corpus.fileids(category):\n",
    "            tokenize_words = corpus.words(fileid)\n",
    "            words_without_stopwords = remove_stopwords(tokenize_words, language='english')\n",
    "            lemmas_words = lemmatizate_text(words_without_stopwords)\n",
    "            local_documents.append((lemmas_words, category))\n",
    "           \n",
    "    random.shuffle(local_documents) \n",
    "    return local_documents"
   ],
   "id": "b5953e195815d6ab",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Часто встречающиеся слова",
   "id": "e7bd35fff4c88f3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.771973Z",
     "start_time": "2025-04-20T13:03:02.768449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def most_common_words(corpus):\n",
    "    tokenize_words = corpus.words()\n",
    "    words_without_stopwords = remove_stopwords(tokenize_words, language='english')\n",
    "    \n",
    "    words_map = FreqDist(words_without_stopwords)\n",
    "    filtered_words_map = [word for word in words_map.keys() if words_map[word] >= 10]\n",
    "    \n",
    "    local_word_features = filtered_words_map[:3000]\n",
    "    \n",
    "    return local_word_features"
   ],
   "id": "2eed43908ce51d10",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Создание вектора признаков",
   "id": "cf39500b079456b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.777124Z",
     "start_time": "2025-04-20T13:03:02.773515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def document_features(document, param_word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "\n",
    "    # Признаки наличия слов из word_features\n",
    "    for word in param_word_features:\n",
    "        features[f'contains({word})'] = (word in document_words)\n",
    "\n",
    "    return features"
   ],
   "id": "c50f794554f59697",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Получение обучающей и тестовых выборок",
   "id": "a8c7d8f20ecc794a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.781815Z",
     "start_time": "2025-04-20T13:03:02.777957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_sets(param_documents):\n",
    "    word_features = most_common_words(reuters)\n",
    "    featuresets = [(document_features(d, word_features), c) for (d, c) in param_documents]\n",
    "\n",
    "    test_size = int(len(featuresets) * 0.1)\n",
    "    train_set, test_set = featuresets[test_size:], featuresets[:test_size]\n",
    "    \n",
    "    return {\n",
    "        'test_size': test_size,\n",
    "        'train_set': train_set,\n",
    "        'test_set': test_set\n",
    "    }"
   ],
   "id": "2e9a1fbef9da0994",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Обучение и получение Наивного Байесовского Классификатора",
   "id": "1300c2a1e89b2dc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.786130Z",
     "start_time": "2025-04-20T13:03:02.782672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_naive_bayes_classifier(param_train_set):\n",
    "    local_nb_classifier = NaiveBayesClassifier.train(param_train_set)\n",
    "    \n",
    "    return local_nb_classifier"
   ],
   "id": "6bfa79b05dbd6633",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Получение точности Наивного Байесовского Классификатора",
   "id": "3e9d5518025a4eef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.790562Z",
     "start_time": "2025-04-20T13:03:02.786939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_accurancy_naive_bayes_classifier(classifier, param_test_set):\n",
    "    local_nb_accuracy = classify.accuracy(classifier, param_test_set)\n",
    "    print(f'Naive Bayes Accuracy: {local_nb_accuracy:.2f}')"
   ],
   "id": "4dce0c6f449ae12e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Получение информативных признаков",
   "id": "a3cbc741e6c2338f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.794744Z",
     "start_time": "2025-04-20T13:03:02.791407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_informative_features(classifier, n):\n",
    "    classifier.show_most_informative_features(n)"
   ],
   "id": "3b0cc32ccf4419cf",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Обучение и получение Дерева Принятия Решений ",
   "id": "d96f6a2bfd22d71c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.800283Z",
     "start_time": "2025-04-20T13:03:02.795584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_decision_tree_classifier(param_train_set):\n",
    "    local_dt_classifier = DecisionTreeClassifier.train(param_train_set)\n",
    "    return local_dt_classifier"
   ],
   "id": "ce20edf70b40433f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Получение точности Дерева Принятия Решений",
   "id": "86b2ca6ddf92ceb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.804757Z",
     "start_time": "2025-04-20T13:03:02.801040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_accurancy_decision_tree_classifier(classifier, param_test_set):\n",
    "    local_dt_accuracy_nltk = classify.accuracy(classifier, param_test_set)\n",
    "    print(f'Decision Tree Accuracy: {local_dt_accuracy_nltk:.2f}')"
   ],
   "id": "e7c5f4a4ef30cc9c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Получение оценки точности при использовании комбинированного классификатора",
   "id": "33d001d8cf8e898b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.809654Z",
     "start_time": "2025-04-20T13:03:02.806276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ensemble_classify(features, param_nb_classifier, param_dt_classifier):\n",
    "    nb_vote = param_nb_classifier.classify(features)\n",
    "    dt_vote = param_dt_classifier.classify(features)\n",
    "    return dt_vote if nb_vote == dt_vote else nb_vote"
   ],
   "id": "78f41723b8394067",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Тестирование",
   "id": "9aa74c87bf3f9be7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:03:02.815018Z",
     "start_time": "2025-04-20T13:03:02.810578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def testing(n, param_test_set, param_nb_classifier, param_dt_classifier):\n",
    "    results = []\n",
    "    \n",
    "    for _ in range(n):\n",
    "        sample = random.choice(param_test_set)\n",
    "        test_features, true_label = sample\n",
    "        nb_result = param_nb_classifier.classify(test_features)\n",
    "        dt_result = param_dt_classifier.classify(test_features)\n",
    "        ensemble_result = ensemble_classify(test_features, param_nb_classifier, param_dt_classifier)\n",
    "    \n",
    "        # Добавляем результаты в список\n",
    "        results.append([\n",
    "            true_label,\n",
    "            nb_result,\n",
    "            dt_result,\n",
    "            ensemble_result\n",
    "        ])\n",
    "        \n",
    "    columns = ['Категория', 'Наивный Байесовский', 'Дерево Решений', 'Комбинированный']\n",
    "    results_df = pd.DataFrame(results, columns=columns)\n",
    "    \n",
    "    return results_df"
   ],
   "id": "4b5941b31b804aa4",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Эксперимент",
   "id": "993d487769393ed9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:42:05.155084Z",
     "start_time": "2025-04-20T13:03:02.815935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "documents = filtration_documents(reuters)\n",
    "sets = get_sets(documents)\n",
    "nb_classifier = get_naive_bayes_classifier(sets['train_set'])\n",
    "dt_classifier = get_decision_tree_classifier(sets['train_set'])"
   ],
   "id": "482eddd63198bf5e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:45:17.071707Z",
     "start_time": "2025-04-20T13:42:05.156Z"
    }
   },
   "cell_type": "code",
   "source": "get_accurancy_naive_bayes_classifier(nb_classifier, sets['test_set'])",
   "id": "641851732e19a106",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.61\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:45:17.390291Z",
     "start_time": "2025-04-20T13:45:17.072593Z"
    }
   },
   "cell_type": "code",
   "source": "get_informative_features(nb_classifier, 10)",
   "id": "948de8cc8774b736",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "        contains(coffee) = True           coffee : earn   =   2360.4 : 1.0\n",
      "          contains(palm) = True           palm-o : earn   =   2337.1 : 1.0\n",
      "       contains(sorghum) = True           sorghu : earn   =   2334.1 : 1.0\n",
      "        contains(nickel) = True           nickel : earn   =   2238.3 : 1.0\n",
      "     contains(economist) = True             rand : earn   =   2073.8 : 1.0\n",
      "          contains(crop) = True           copra- : earn   =   1481.2 : 1.0\n",
      "         contains(juice) = True           orange : earn   =   1412.9 : 1.0\n",
      "          contains(rice) = True             rice : acq    =   1389.1 : 1.0\n",
      "         contains(cargo) = True           propan : earn   =   1382.5 : 1.0\n",
      "       contains(naphtha) = True           naphth : acq    =   1324.1 : 1.0\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:45:17.417043Z",
     "start_time": "2025-04-20T13:45:17.391463Z"
    }
   },
   "cell_type": "code",
   "source": "get_accurancy_decision_tree_classifier(dt_classifier, sets['test_set'])",
   "id": "1b2cc64500b36c1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.58\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:45:23.024738Z",
     "start_time": "2025-04-20T13:45:17.418057Z"
    }
   },
   "cell_type": "code",
   "source": "testing(20, sets['test_set'], nb_classifier, dt_classifier)",
   "id": "bdfd772ae13c1b20",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Категория Наивный Байесовский Дерево Решений Комбинированный\n",
       "0      trade               trade          trade           trade\n",
       "1        acq                 acq            acq             acq\n",
       "2   interest            interest            dlr        interest\n",
       "3    sorghum              barley          grain          barley\n",
       "4        acq                 acq            acq             acq\n",
       "5       earn                earn           earn            earn\n",
       "6   interest            interest       interest        interest\n",
       "7      grain               wheat          grain           wheat\n",
       "8       earn                earn           earn            earn\n",
       "9     coffee              coffee         coffee          coffee\n",
       "10  money-fx            money-fx       money-fx        money-fx\n",
       "11  interest            money-fx       money-fx        money-fx\n",
       "12       bop                 bop       money-fx             bop\n",
       "13      earn                earn           earn            earn\n",
       "14   carcass               grain        carcass           grain\n",
       "15      earn                 gnp            acq             gnp\n",
       "16      earn                earn           earn            earn\n",
       "17  soy-meal            palm-oil        veg-oil        palm-oil\n",
       "18      earn                earn           earn            earn\n",
       "19     grain                 gas           corn             gas"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Категория</th>\n",
       "      <th>Наивный Байесовский</th>\n",
       "      <th>Дерево Решений</th>\n",
       "      <th>Комбинированный</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trade</td>\n",
       "      <td>trade</td>\n",
       "      <td>trade</td>\n",
       "      <td>trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acq</td>\n",
       "      <td>acq</td>\n",
       "      <td>acq</td>\n",
       "      <td>acq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interest</td>\n",
       "      <td>interest</td>\n",
       "      <td>dlr</td>\n",
       "      <td>interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sorghum</td>\n",
       "      <td>barley</td>\n",
       "      <td>grain</td>\n",
       "      <td>barley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acq</td>\n",
       "      <td>acq</td>\n",
       "      <td>acq</td>\n",
       "      <td>acq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>interest</td>\n",
       "      <td>interest</td>\n",
       "      <td>interest</td>\n",
       "      <td>interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>grain</td>\n",
       "      <td>wheat</td>\n",
       "      <td>grain</td>\n",
       "      <td>wheat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>money-fx</td>\n",
       "      <td>money-fx</td>\n",
       "      <td>money-fx</td>\n",
       "      <td>money-fx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>interest</td>\n",
       "      <td>money-fx</td>\n",
       "      <td>money-fx</td>\n",
       "      <td>money-fx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bop</td>\n",
       "      <td>bop</td>\n",
       "      <td>money-fx</td>\n",
       "      <td>bop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>carcass</td>\n",
       "      <td>grain</td>\n",
       "      <td>carcass</td>\n",
       "      <td>grain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>earn</td>\n",
       "      <td>gnp</td>\n",
       "      <td>acq</td>\n",
       "      <td>gnp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>soy-meal</td>\n",
       "      <td>palm-oil</td>\n",
       "      <td>veg-oil</td>\n",
       "      <td>palm-oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>grain</td>\n",
       "      <td>gas</td>\n",
       "      <td>corn</td>\n",
       "      <td>gas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
