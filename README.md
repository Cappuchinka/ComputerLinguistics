# Компьютерная лингвистика
## Лабораторная работа №1
### Работа с библиотекой Natural Language Toolkit (NLTK) для решения задач статистической обработки текстов (английский язык)

1 – этап экспериментов (Анализ структуры отдельного корпуса (текста))

1) Выбрать корпус для экспериментов (из NLTK или загрузить свой)
2) Провести статистический анализ текста:

   * Длина текста, словарь текста, число различных слов в словаре, рассчитать параметр лексического разнообразия текста.
   * Определить число предложений, слов (провести токенизацию).
   * Убрать стоп слова (предлоги, союзы, управляющие слова) и построить частотный график встречаемости слов в тексте. Кумулятивный график частотного распределения слов.
   * Выделить частотные слова, относящиеся к одной леме (провести лематизацию)
   * На основе результатов лематизации вывести на печать слова, определяющие тематику текста (претенденты на ключевые слова). Выделить по частоте и длине.
   * Провести исследование тематической структуры текста (в каких частях текста о чем говориться) – исследовать частотное расположение слов в тексте - построить график дисперсии.
   * Распечатать ключевые слова (частотные слова), относящиеся к наиболее тематически важному разделу текста (определить по графику дисперсии). Для них построить частотный график встречаемости слов в тексте. Кумулятивный график частотного распределения слов.
   * Для ключевых слов найти им соответствующие биграммы и коллокации в тексте, оценить их частотность. Экспертным методом проверить соответствуют ли определенные словосочетания важными для уточнения тематики текста. 

2 – этап экспериментов (сравнительный анализ нескольких корпусов)

1) Выбрать 2-3 корпуса для экспериментов (из NLTK или загрузить свои)
2) Провести статистический анализ этих корпусов по плану задания 1.
   * Провести сравнение по статистическим параметрам: словарь текста, число различных слов в словаре, рассчитать параметр лексического разнообразия текста.
   * Исследовать тематические структуры текстов (в каких частях текстов о чем говориться) – исследовать частотное расположение слов в тексте - построить график дисперсии.

## Лабораторная работа №2
### Исследование текстового корпуса с помощью инструментов библиотеки NLTK

## Лабораторная работа №3
### Классификация текстов корпуса с помощью инструментов библиотеки NLTK

## Лабораторная работа №4
### Работа с регулярными выражениями
Используя регулярные выражения, найти и получить элементы текста в соответствии с шаблонами

## Лабораторная работа №5
### Предобработка текста на русском языке (библиотека Наташа)

1. Выберите датасет из предложенных вариантов:

    * [Russian Social Media Text Classification](https://www.kaggle.com/datasets/mikhailma/russian-social-media-text-classification)
    * [Russian Sentiment Emotion Datasets](https://github.com/searayeah/russian-sentiment-emotion-datasets/tree/main)
    * [RusAge: Corpus for Age-Based Text Classification](https://www.kaggle.com/datasets/oldaandozerskaya/fiction-corpus-for-agebased-text-classification)

2. Проведите анализ датасета:

    * Постройте график распределения классов
    * Найдите статистику по выбранному корпусу текстов
    * Проведите токенизацию, лемматизацию, стемминг с помощью библиотеки ```natasha```
    * Проведите очистку данных с помощью удаления стоп-слов и регулярных выражений
    * Выделите ключевые слова и постройте график дисперсии

3. Полученные преобразования по токенизации и очистке текста представьте в виде sklearn pipeline.

## Лабораторная работа №6
### Bag-of-Words, TF-IDF, Co-occurrence Matrix

1. Разделите датасет, выбранный в прошлом задании, на обучающую и тестовую выборки.

2. Выделите признаки из корпуса текстов с помощью мешка слов. Обучите несколько моделей, оцените результат с помощью Accuracy, Precision, Recall, F1-score. Постройте биграммы и сравните результаты обучения с обычным мешком слов.

3. Выделите признаки из корпуса текстов с помощью TF-IDF. Обучите несколько моделей, оцените результат с помощью метрик. Постройте биграммы и сравните результаты обучения с обычным TF-IDF.

4. Постройте матрицу встречаемости слов для вашего обучающего датасета.

## Лабораторная работа №7
### Word2Vec

1. Постройте Skip-Gram Word2Vec модель для вашего датасета.

2. Визуализируйте эмбеддинги для первых 50 слов. Что можно сказать о полученном графике?

3. Постройте эмбеддинги для текстов и обучите модели классификации. Оцените качество их работы.

4. Постройте график зависимости качетсва работы моделей от размеров контекстного окна.

5. Повторите 2-4 для CBOW-модели. Как повлияла смена модели построения эмбеддингов на качество моделей классификации?

## Лабораторная работа №8
### Yargy-Parser

Для смысловых шаблонов, соответствующих своему варианту разработать грамматики для Томита-парсера для извлечения соответствующих фактов из текста. Постарайтесь на возможно подробном для вас уровне извлечь информацию.
Корпус текстов для разработки грамматик и тестирования 

https://polpred.com/news/

К каждому заданию приведено несколько предложений, в качестве примеров, на которые можно ориентироваться в процессе разработки грамматик. Рекомендуется найти в новостных сообщениях похожие по смыслу предложения, но использующие другие синтаксические структуры и лексические инварианты.
Цветом выделены факты, которые надо найти с помощью грамматик.
Курсивом в каждом задании представлена формула – шаблон для поиска фактов.
